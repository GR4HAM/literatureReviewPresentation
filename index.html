
<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Generating and Understanding Multi-layered Music</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/serif.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h2>Generating and Understanding Multi-layered Music</h2>
					<h4>Midyear presentation</h4> 
					<p> Graham Spinks, February 2017</p>					
				</section>
				<section>
					<h2>PART 1</h2>	
					<h3>Existing approaches (literature review)</h3>								
				</section>

					<section>
						<h2>Non-neural network</h2>
							<ul>
								<li><b>Machine learning</b>: 1. Deconstruction, 2. common signatures 3. compatibilty (recombination) <a href="#/reference-slide-1">[1]</a> </li>
								<li><b>Human - Machine interaction with statistical ML</b>,  <a href="http://www.flow-machines.com/ai-makes-pop-music/">"Daddy's Car" in the style of the beatles</a> , <a href="https://www.csl.sony.fr/music.php">Sony CSL</a></li>
							</ul>

							
					
					</section>

				<section>	
					<section>
						<h3>RNN's are most suitable for variable-length input</h3>
						<p>But: vanishing gradient problem </p>	
						<ul>
							<li>Music events often bridge several time steps: chord changes, chorus, ... </li>	
							<li>-> how can we model long-term dependencies</li>	
							<li>Solution: LSTM / RBM / Attention models / GRU's </li>	
						</ul>			
					</section>

					<section>
						<h2>RNN approaches</h2>
						
						<ul>
							<li>Character based (abc notation) -> monophonic <a href="#/reference-slide-1">[3]</a> </li>
							<li>Custom representation for blues LSTM - similar to midi representation -> polyphonic <a href="#/reference-slide-1">[4]</a> </li>
							<li>Bi-axial RNN: connect over time and over notes <a href="#/reference-slide-1">[5]</a> </li>
							<li>GRU's: Gated Recurrent Unit: similar to LSTM without separate memory cell <a href="#/reference-slide-1">[6]</a> </li>
						</ul>				
					</section>

					<section>
						<h2>RNN-RBM</h2>
						
							<p>RNN-RBM: Restricted Boltzmann Machine to predict conditional distribution of next timestep given previous timesteps <a href="#/reference-slide-1">[7], [8]</a> </p>
							<p>Polyphonic and nice sounding music but no sense of time and only plays a couple of chords</p>
					
					</section>


					<section>
						<h2>Reinforcement learning</h2>
					
						<p>RL tuner by Magenta: creates RL reward function that teaches the LTSM model to follow certain rules<a href="#/reference-slide-1">[9]</a> </p>
						<p>Tries to solve problem of excessively repeated tokens. </p>
						<blockquote>"Any beginning music student learns that groups of notes belong to keys, chords follow progressions, and songs have consistent structures made up of musical phrases."</blockquote>
					</section>
					<section>
						<img src="pics/RL_RNN.png" >
					</section>
					

					<section>
						<h2>DeepBach</h2>
					
						<p>Given a melody, create convincing harmonies, trained on bach chorales <a href="#/reference-slide-1">[10]</a> </p>
						<p>Uses known information of past and future melody to create harmony for the current time step. </p>
						<p>Good results but limited flexibility due to chosen data representation. </p>
					</section>
					<section>
						<img src="pics/deepBach.png" />
						<p>DeepBach <a href="#/reference-slide-1">[10]</a> </p>
					</section>
					<section>	
						<iframe width="560" height="315" src="https://www.youtube.com/embed/QiBM7-5hA6o" frameborder="0" allowfullscreen></iframe>		  
					</section>

				</section>

				<section>
					<section>
						<h3>Other approaches: Non-symbolic audio</h3>

						<p> VRNN: RNN combined with variational autoencoder <a href="#/reference-slide-1">[40]</a> </p>
						<img src="pics/VRNN.png" height="350" alt="hi" class="inline"/>
					</section>

					<section>
						<h3>Other approaches: Non-symbolic audio</h3>						
						<p> Wavenet: fully convolutional Neural Network <a href="#/reference-slide-1">[41]</a> </p>
						<iframe width="450" height="300" src="https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif" frameborder="0" allowfullscreen></iframe>	
						<iframe width="450" height="300" src="https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig1-Anim-160908-r01.gif" frameborder="0" allowfullscreen></iframe>	
						
					</section>

					<section>
						<h3>Other approaches: Images</h3>
						<p>Image generation: usually CNN</p>

						<ul>
							<li>Deep convolutional nets (eg Deep Dream <a href="#/reference-slide-1">[15]</a> ) </li>
							<li> Style reconstruction by computing correlations in layers of CNN <a href="#/reference-slide-1">[20]</a>  </li>
						</ul>				
					</section>

					
					<section>
						<h3>Other approaches: Images</h3>

						<p> DRAW: Deep Recurrent Attentive Writer</p>
						<p> LSTM sequentially draws an immage with fully differentiable attention network <a href="#/reference-slide-1">[19]</a>, <a href="#/reference-slide-1">[21]</a>  </p>	<iframe width="560" height="315" src="http://karpathy.github.io/assets/rnn/house_generate.gif" frameborder="0" allowfullscreen></iframe>	

					</section>

				</section>

				
				<section>
					<section>
						
						<h3>Training LSTM's</h3>
						<p> Exploding gradient rather than vanishing gradient <a href="#/reference-slide-1">[30]</a> </p>
						<ul>
							<li>Impose a hard constraint on the norm of the gradient</li>	
							<li>Ensure that minibatches are more or less the same size for training speedup</li>	
						</ul>				
					</section>
					<section>
						<h3>Recurrent batch normalization</h3>
						<p>Leads to faster convergence and improved generalization <a href="#/reference-slide-1">[31]</a></p>	
						<img src="pics/batch_norm_RNN.png" >					
					</section>
					<section>
						<h3>Weight normalization</h3>
						<p>Similar to batch normalization but simpler and less computational overhead <a href="#/reference-slide-1">[32]</a></p>	
						<img src="pics/weight_norm_RNN.png" >					
					</section>
				</section>	

				<section>
					<section>
						<h3>Generative Adverserial Networks</h3>
						<p> Train two models in a minimax game <a href="#/reference-slide-1">[50]</a>: </p>	
						<br>
						<p>A generative model G that captures the data distribution</p>
						<p>A discriminative model D that estimates the probability that a sample came from the training data rather than G</p>
						<img src="pics/formula_GAN.png" >	
					</section>
					<section>						
						<p>Alternative versions: LAPGAN <a href="#/reference-slide-1">[51]</a>, DCGAN <a href="#/reference-slide-1">[52]</a> </p>
						<p>Achieve good results for image generation</p>
						<img src="pics/architecture_DCGAN.png" >	
					</section>	
					<section>						
						<p>The learned represenation space has smooth transitions   <a href="#/reference-slide-1">[52]</a> </p>
						<img src="pics/transitions_DCGAN.png" >	
					</section>	
					<section>
						<p>Not easy to train adverserial models with recurrent networks as the discriminator tends to determine whether it's a fake after just a few timesteps</p>
						<p>Recently, a C-RNN-GAN has been trained, modeling the whole joint probability of a sequence, for real-valued data (midi) <a href="#/reference-slide-1">[53]</a></p>						
						<img src="pics/architecture_C-RNN-GAN.png" >	
					</section>	

					<section>
						<p>So far, GAN's have only been applied to real-valued data, as the gradient of the loss from D is used to adapt the output of G in the right direction</p>
						<p> SeqGAN was developed for sequences of symbols; generation is considered a sequential decision making process with RL <a href="#/reference-slide-1">[54]</a></p>		
						<p> This approach requires a task-specific sequence score (eg BLEU in MT) for the reward</p>		
						<img src="pics/SeqGAN.png" >	
					</section>	
					<section>
						<p>SeqGAN: LSTM for G, CNN for D, but other architectures are possible</p>
						<p>Pretraining with ML, then alternative training of G and D</p>			
						<img src="pics/SeqGAN2.png" >	
					</section>	

					<section>
						<p>Adverserial Professor Teaching: iteratively run generator freely and with input sequence <a href="#/reference-slide-1">[55]</a></p>
						<p>Avoid gradient problem of discrete output by training discriminator on hidden states </p>	
						<p>Tries to solve: for longer sequences RNN's tend to diverge to state spaces which were never observed during training</p>		
						<img src="pics/architecture_professor_forcing.png" >	
					</section>	

				</section>


				<section>
					<h2>Frequently used data sources </h2>
					<ul>
						<li><b>Piano-midi.de</b> is a classical piano MIDI archive that
was split according to Poliner & Ellis (2007). </li>	
						<li><b>Nottingham</b> is a collection of 1200 folk tunes with
chords instantiated from the ABC format</li>	
						<li><b>MuseData</b> is an electronic library of orchestral and
piano classical music from CCARH.</li>	
						<li><b>JSB chorales</b> refers to the entire corpus of 382 fourpart
harmonized chorales by J. S. Bach with the
split of Allan & Williams (2005).</li>	
					</ul>					
				</section>


				<section>
					<h3>Evaluating the results</h3>
					<ul>
						<li>Listening tests (Mechanical Turk)</li>	
						<li>Battle of GAN's</li>	
						<li>Plagiarism tests</li>	
						<li>BLEU sequence score <a href="#/reference-slide-1">[54]</a></li>	
					</ul>					
				</section>



				<section>
					<section>
						<h3>Understanding neural networks</h3>
						<p>Visualizing the "neuron" firings in RNN's <a href="#/reference-slide-1">[21]</a> </p>	
						<img src="pics/neuron_firing.png" >	
					</section>
					<section>						
						<p> Activation maximization: find the image tha maximally activates a certain neuron <a href="#/reference-slide-1">[22]</a> </p>
						<img src="pics/activation_max.png" >	
					</section>	
					<section>						
						<p> Code inversion: synthesize an image that produces similar activation vector as a real image at a particular layer  <a href="#/reference-slide-1">[23]</a> </p>
						<img src="pics/code_inversion.png" height="400" >	
					</section>	
					<section>
						<p>Inproved activation maximization: Multifaceted feature visualization <a href="#/reference-slide-1">[24]</a></p>						
						<img src="pics/multifaceted_feature.png" >	
					</section>				
				</section>

	
				<section>
					<h2>PART 2</h2>	
					<h3>Proposed approach </h3>								
				</section>

				<section>
					<section>
						
						<h3>Representation</h3>
						<p>Represent music as a sequence of words:  <a href="http://www.humdrum.org/">kern representation (humdrum)</a></p>
						 <img src="pics/representation2_humdrum.png" height="400"> <img src="pics/representation_humdrum.png" height="400">	
											
					</section>
					<section>	
						<p>Downside of kern representation: note intensity levels that are present in MIDI notation are lost  </p>	
						<p>Possible follow up: attempt same approach with MIDI representation </p>					
						<p>Specific datasets for scientific research, both monophonic and polyphonic: <a href="http://kern.ccarh.org/">http://kern.ccarh.org/</a>  </p>							
					</section>
					<section>						
						<p>Use word embeddings to represent the music in a dense, continuous space </p>								
						<p>Existing NN algorithms: CBOW, Skip-Gram model <a href="#/reference-slide-1">[60]</a> </p>	
						<img src="pics/architecture_embeddings.png" >					
					</section>
					<section>
						<p>We expect similar notes to  be represented near eachother <a href="https://www.tensorflow.org/tutorials/word2vec/">[source]</a> </p>							
						<img src="pics/visualization_embeddings.png" >					
					</section>
					<section>						
						<p>As word embeddings have been useful in many NLP tasks, we expect the same for music generation </p>								
						<p>For this thesis we propose two possible implementations for music generation: SeqGAN and Adverserial Professor Teaching with word embeddings</p>	
				
					</section>
					<section>
						<p>SeqGAN model with word embedding representation; gradient is computed over continuous space gradient rather than policy  gradient; faster convergence expected	</p>
						<p>Eg. LSTM for G, CNN for D, but other architectures are possible</p>
						<p>Pretraining with ML, then alternative training of G and D</p>	

						<img src="pics/SeqGAN2.png" >	
					</section>	

					<section>
						<p>Adverserial Professor Teaching: iteratively run generator freely and with input sequence <a href="#/reference-slide-1">[55]</a></p>
						<p>Adverserial Professor Teaching with word embeddings: no need to train over hidden states; faster and better convergence expected </p>		
						<img src="pics/architecture_professor_forcing.png" >	
					</section>	
					<section>						
						<p>Subsequent analysis of learned features by neuron visualization  <a href="#/reference-slide-1">[21]</a> </p>	
						<img src="pics/neuron_firing.png" >	
				
					</section>

					<section>						
						<p>In the case of Professor Teaching we expect to be able to explore the state space <a href="#/reference-slide-1">[52]</a> </p>	
						<img src="pics/transitions_DCGAN.png" >	
				
					</section>


					
				</section>	


				<section id="reference-slide-1">
					<h3>References</h3>
					<ul class="smallFont">
						<li> [1] <a href="http://artsites.ucsc.edu/faculty/cope/experiments.htm"> Experiments in Musical Intelligence </a> </li>
						<li> [3] <a href="https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/"> https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/ </a> </li>
						<li>[4] <a href="http://people.idsia.ch/~juergen/blues/IDSIA-07-02.pdf"> A First Look at Music Composition
						using LSTM Recurrent Neural Networks, Douglas Eck, Jurgen Schmidhuber </a> </li>
						<li>[5] <a href="http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/"> Composing Music With Recurrent Neural Networks, Daniel Johnson </a> </li>
						<li>[6] <a href="https://arxiv.org/pdf/1412.3555.pdf"> Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio </a> </li>
						<li>[7] <a href="http://deeplearning.net/tutorial/rnnrbm.html#rnnrbm"> Modeling and generating sequences of polyphonic music with the RNN-RBM, Sid Sigtia </a> </li>
						<li>[8] <a href="http://www-etud.iro.umontreal.ca/~boulanni/ICML2012.pdf"> Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription, Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent </a> </li>
						<li>[9] <a href="https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning/"> Tuning Recurrent Neural Networks with Reinforcement Learning, Natasha Jaques (Magenta) </a> </li>
						<li>[10] <a href="https://arxiv.org/pdf/1612.01010.pdf"> DeepBach: a Steerable Model for Bach chorales generation, Gaetan Hadjeres, Francois Pachet </a> </li>
						<li>[15] <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"> Inceptionism: Going Deeper into Neural Networks, Alexander Mordvintsev, Christopher Olah, Mike Tyka </a> </li>
						<li>[19] <a href="https://arxiv.org/pdf/1502.04623.pdf"> DRAW: A Recurrent Neural Network For Image Generation, Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra </a> </li>
						<li>[20] <a href="https://arxiv.org/pdf/1508.06576v1.pdf"> A Neural Algorithm of Artistic Style, Leon A. Gatys, Alexander S. Ecker, Matthias Bethge </a> </li>
						<li>[21] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"> The unreasonable effectiveness of recurrent neural networks, Andrej Karpathy </a> </li>
						<li>[22] <a href="http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf"> Visualizing Higher-Layer Features of a Deep Network, Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent  </a> </li>
						<li>[23] <a href="https://arxiv.org/pdf/1512.02017.pdf"> Visualizing deep convolutional neural networks using natural pre-images, Aravindh Mahendran, Andrea Vedaldi </a> </li>
						<li>[24] <a href="https://arxiv.org/pdf/1602.03616.pdf"> Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks, Anh Nguyen, Jason Yosinski, Jeff Clune  </a> </li>
						<li>[30] <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"> Sequence to sequence learning with neural networks, Ilya Sutskever, Oriol Vinyals, Quoc V. Le </a> </li>
						<li>[31] <a href="http://www.gitxiv.com/posts/MwSDm6A4wPG7TcuPZ/recurrent-batch-normalization"> Recurrent Batch Normalization, Tim Cooijmans, Nicolas Ballas, Cesar Laurent, Caglar Guelcehre </a> </li>
						<li>[32] <a href="https://arxiv.org/pdf/1602.07868.pdf"> Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks, Tim Salimans, Diederik P. Kingma  </a> </li>
						<li>[40] <a href="https://arxiv.org/pdf/1506.02216.pdf"> A Recurrent Latent Variable Model for Sequential Data, Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio </a> </li>
						<li>[41] <a href="https://arxiv.org/pdf/1609.03499.pdf"> Wavenet: a generative model for raw audio, Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu   </a> </li>
						<li>[50] <a href="https://arxiv.org/pdf/1406.2661.pdf"> Generative Adversarial Networks,  Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio</a> </li>
						<li>[51] <a href="https://arxiv.org/pdf/1506.05751.pdf"> Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks, Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus </a> </li>
						<li>[52] <a href="https://arxiv.org/pdf/1511.06434.pdf"> Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, Alec Radford, Luke Metz, Soumith Chintala </a> </li>
						<li>[53] <a href="https://arxiv.org/pdf/1611.09904.pdf"> C-RNN-GAN: Continuous recurrent neural networks with adversarial training, Olof Mogren </a> </li>
						<li>[54] <a href="https://arxiv.org/pdf/1609.05473.pdf"> SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient, Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu </a> </li>
						<li>[55] <a href="https://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf"> Professor Forcing: A New Algorithm for Training Recurrent Networks, Anirudh Goyal, Alex Lamb, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio </a> </li>
						<li>[60] <a href="https://arxiv.org/pdf/1301.3781.pdf"> Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean </a> </li>
					</ul>		
				</section>

				<!-- TODO: 
				* evaluating the results 
				* look in more detail at SeqGAN and Professor teaching
				* parallel GAN training?
			-->

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

		<!-- Everything below this point is only used for the reveal.js demo page -->
		<!--
		<a class="fork-reveal" style="display: none;" href="https://github.com/hakimel/reveal.js"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github-camo.global.ssl.fastly.net/365986a132ccd6a44c23a9169022c0b5c890c387/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f7265645f6161303030302e706e67" alt="Fork reveal.js on GitHub"></a>

		<div class="share-reveal" style="display: none; position: absolute; bottom: 16px; left: 50%; margin-left: -139px; z-index: 20;">
			<a class="share-reveal-editor" href="http://slides.com">Try the online editor</a>

			<a class="share-reveal-facebook" href="http://www.facebook.com/sharer.php?u=http%3A%2F%2Flab.hakim.se%2Freveal-js">
				<svg viewBox="-8 -8 48 48" width="20" height="20" version="1.1" xmlns="http://www.w3.org/2000/svg">
					<path fill="#FFFFFF" d="M25.613-4.557c0,0-3.707,0-6.166,0c-3.662,0-7.732,1.535-7.732,6.835c0.019,1.845,0,3.613,0,5.603H7.481 v6.728h4.366v19.37h8.021V14.48h5.295l0.479-6.618h-5.913c0,0,0.016-2.946,0-3.8c0-2.093,2.184-1.974,2.312-1.974 c1.042,0,3.059,0.003,3.578,0v-6.646H25.613z"/>
				</svg>
			</a>

			<a class="share-reveal-twitter" href="http://twitter.com/share?url=http%3A%2F%2Flab.hakim.se%2Freveal-js&text=reveal.js%20-%20The%20HTML%20presentation%20framework&via=revealjs&related=revealjs">
				<svg viewbox="0 0 2000 1625.36" width="20" height="20" version="1.1" xmlns="http://www.w3.org/2000/svg">
					<path d="m 1999.9999,192.4 c -73.58,32.64 -152.67,54.69 -235.66,64.61 84.7,-50.78 149.77,-131.19 180.41,-227.01 -79.29,47.03 -167.1,81.17 -260.57,99.57 C 1609.3399,49.82 1502.6999,0 1384.6799,0 c -226.6,0 -410.328,183.71 -410.328,410.31 0,32.16 3.628,63.48 10.625,93.51 -341.016,-17.11 -643.368,-180.47 -845.739,-428.72 -35.324,60.6 -55.5583,131.09 -55.5583,206.29 0,142.36 72.4373,267.95 182.5433,341.53 -67.262,-2.13 -130.535,-20.59 -185.8519,-51.32 -0.039,1.71 -0.039,3.42 -0.039,5.16 0,198.803 141.441,364.635 329.145,402.342 -34.426,9.375 -70.676,14.395 -108.098,14.395 -26.441,0 -52.145,-2.578 -77.203,-7.364 52.215,163.008 203.75,281.649 383.304,284.946 -140.429,110.062 -317.351,175.66 -509.5972,175.66 -33.1211,0 -65.7851,-1.949 -97.8828,-5.738 181.586,116.4176 397.27,184.359 628.988,184.359 754.732,0 1167.462,-625.238 1167.462,-1167.47 0,-17.79 -0.41,-35.48 -1.2,-53.08 80.1799,-57.86 149.7399,-130.12 204.7499,-212.41" style="fill:#ffffff"/>
				</svg>
			</a>
		</div>

		<style>
			/* Social sharing */
			.share-reveal a {
				display: inline-block;
				height: 34px;
				line-height: 32px;
				padding: 0 10px;
				color: #fff;
				font-family: Helvetica, sans-serif;
				text-decoration: none;
				font-weight: bold;
				font-size: 12px;
				vertical-align: top;
				text-transform: uppercase;
				box-sizing: border-box;
			}

			.share-reveal .share-reveal-editor {
				line-height: 30px;
			}

			.share-reveal svg {
				vertical-align: middle;
			}

			.share-reveal a + a {
				margin-left: 10px;
			}

			.share-reveal-editor {
				border: 2px solid #fff;
			}

			.share-reveal-twitter,
			.share-reveal-follow {
				background-color: #00aced;
			}

			.share-reveal-facebook {
				background-color: #4B71B8;
			}

			/* Advertising */
			#carbonads {
				width: 370px;
				min-height: 100px;
				font-size: 18px;
				border: 1px solid rgba(255, 255, 255, 0.2);
				padding: 10px;
				margin: 40px auto 0 auto;
				font-size: 16px;
				z-index: 10;
				text-align: left;
			}

			#carbonads .carbon-img img {
				float: left;
				margin: 0 10px 0 0;
				border: 0;
				box-shadow: none;
			}

			#carbonads .carbon-poweredby {
				display: block;
				margin-top: 10px;
				color: #aaa;
			}
		</style>

		<script>
		var _gaq = [['_setAccount', 'UA-15240703-1'], ['_trackPageview']];
		(function(d, t) {
		var g = d.createElement(t),
			s = d.getElementsByTagName(t)[0];
		g.async = true;
		g.src = ('https:' == location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		s.parentNode.insertBefore(g, s);
		})(document, 'script');
		</script>

		<script>
			[].slice.call( document.querySelectorAll( '.share-reveal-facebook, .share-reveal-twitter' ) ).forEach( function( element ) {
				element.addEventListener( 'click', function( event ) {
					event.preventDefault();
					var width = 500, height = 300;
					var winTop = window.screenY + (window.screen.height / 2) - (height / 2);
					var winLeft = window.screenX + (window.screen.width / 2) - (width / 2);
					window.open(this.href, 'shre', 'top=' + winTop + ',left=' + winLeft + ',toolbar=0,status=0,width=' + width + ',height=' + height);
				} );
			} );

			if( !navigator.userAgent.match( /(iphone|android)/gi ) && !!document.querySelector ) {
				document.querySelector( '.share-reveal' ).style.display = 'block';
				document.querySelector( '.fork-reveal' ).style.display = 'block';
			}
		</script>
		-->
	</body>
</html>
